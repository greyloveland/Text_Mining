# -*- coding: utf-8 -*-
"""Text_Mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12naI98m4zlKw_T9qpP6nWm419WbKGbHI

## Text Mining Project
"""

# Packages
!pip install contractions

import pandas as pd
import nltk
import re
import numpy as np
import contractions
import networkx as nx
import seaborn as sns
import matplotlib.pyplot as plt

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


# From Sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer

df = pd.read_csv('df_file.csv')
df.info()

# Did this to see how many values are in each label and how many values we are dealing with
# as well to see if we have even distribution

df['Label'].value_counts()

# To look at data and see what we are working with

df.head()

# Expanding the contractions

def expanding_contractions(text):
  return contractions.fix(text)

expanded_corpus = np.vectorize(expanding_contractions)
df['expanded_text'] = expanded_corpus(df['Text'])

# Lowercasing
# removing special characters
# stop word removal
# Stripping whitespace
# Tokenization

stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
  doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I|re.A)
  doc = doc.lower()
  doc = doc.strip()
  tokens = nltk.word_tokenize(doc)
  filtered_tokens = [token for token in tokens if token not in stop_words]
  doc = ' '.join(filtered_tokens)
  return doc

normalize_corpus = np.vectorize(normalize_document)
norm_corpus = normalize_corpus(df['expanded_text'])
len(norm_corpus)

# Looked at the cleaning process to see if there is anything else that I should take out or maybe put back in

norm_corpus[0:2]

"""## Feature Engineering"""

vectorizer = TfidfVectorizer(
    stop_words='english',
    ngram_range=(1,2),
    max_df=.8,
    min_df=2
)

X = vectorizer.fit_transform(norm_corpus)
y = df['Label']

# Splitting Data into Text and Labels
texts = df['Text'].tolist()
labels = df['Label'].tolist()

X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)

# This forces the train and test to have the same number of features

X_train_vec = vectorizer.transform(X_train)
X_test_vec = vectorizer.transform(X_test)

num_features_train = X_train_vec.shape[1]
num_features_test = X_test_vec.shape[1]
print(f"Number of features in training set: {num_features_train}")
print(f"Number of features in test set: {num_features_test}")

# Data Models

models = {
    'Naive Bayes' : MultinomialNB(),
    'Linear SVC' : LinearSVC(),
    'Random Forest' : RandomForestClassifier()
}

# Performing Cross validation for each model which will help overfitting
for name, model in models.items():
  scores = cross_val_score(model, X_train_vec, y_train, cv=5)
  print(f"Cross-Validation Mean Accuracy: {scores.mean():.4f}")

param_grid_svc = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'loss': ['hinge','squared_hinge'],
    'max_iter': [1000, 5000, 10000]
}

# Grid Search for Linear SVC (Support Vector Classifier)
# Reason I am doing this is because when looking at the cross validations accuracy it was performing the best
grid_search_svc = GridSearchCV(LinearSVC(random_state = 42), param_grid_svc, cv=5, verbose = 10)
grid_search_svc.fit(X_train_vec, y_train)

# The best parameters for my best performing model
# Given the parameters I had it go through. With more time and computer power I could maybe find better results.

print("Best parameters for LinearSVC:", grid_search_svc.best_params_)

y_pred = grid_search_svc.predict(X_test_vec)

# Computer Matrix

cm = confusion_matrix(y_test, y_pred)
labels = sorted(list(set(y_test)))

# plot the heatmap
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, y_pred))

"""### Unsupervised Learning"""

# Unsupervised Learning kmeans clustering
# Divided it into 10 different clusters

kmeans = KMeans(n_clusters = 10, random_state = 42)
kmeans.fit(X)
labels = kmeans.labels_

df['Cluster'] = kmeans.labels_
df.groupby('Cluster').head()

# Make it 2D
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X.toarray())

# Get Cluster Labels
lables = kmeans.labels_
num_clusters = len(set(labels))

# Plot
plt.figure(figsize=(8,6))
for cluster_id in range(num_clusters):
  idx = np.where(labels == cluster_id)
  plt.scatter(X_reduced[idx, 0], X_reduced[idx, 1], label=f'Cluster {cluster_id}', s=30)

plt.title("KMeans Clustering")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.grid(True)
plt.show()

# Vectorize
vectorizer = TfidfVectorizer(
    stop_words='english',
    max_features = 76814
)
X = vectorizer.fit_transform(norm_corpus)

# train Kmeans on this same X
kmeans = KMeans(n_clusters = 5, random_state = 42)
kmeans.fit(X)

order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()

for i in range(kmeans.n_clusters):
  print(f"Cluster {i}:")
  top_terms = [terms[ind]for ind in order_centroids[i, :10]]
  print("Top terms: ", top_terms)
  print()

"""### Document Summarization"""

def summarize_article(text, top_n = 3):
  # 1
  sentence = sent_tokenize(text)
  if len(sentence) == top_n:
    return text
  # 2
  vectorizer = TfidfVectorizer()
  sentence_vectors = vectorizer.fit_transform(sentence)
  # 3
  sim_matrix = cosine_similarity(sentence_vectors)
  # 4
  scores = nx.pagerank(nx.from_numpy_array(sim_matrix))
  # 5
  ranked_sentences = sorted(((scores[i], s)for i, s in enumerate(sentence)), reverse=True)
  # 6
  summary = " ".join([s for _, s in ranked_sentences[:top_n]])
  return summary

'''
Step 1: Sentence Tokenization
Step 2: TF-IDF Vectorization
Step 3: Cosine Similarity Matrix
Step 4: Build a similarity graph and apply PageRank
Step 5: Rank Sentence
Step 6: Extract Top N Sentences
'''

# We are splitting articles into sentences using nltk, then we use TF-IDF to vector each word and if it relates to the article.
# The cosine similarity matrix tells us how related the senteces are. High value sentences are similar low value they are different.
# We also use pagerank to assign importance and and how well everything is connected. We then grab the top 3 highest importance sentences.

summaries = [summarize_article(article, top_n=3) for article in norm_corpus]

for i, summary in enumerate(summaries):
    print(f"--- Summary for Article {i+1} ---\n{summary}\n")

# Creates a new column called summary and then saves it to a CSV file

df['Summary'] = [summarize_article(text) for text in df['Text']]
df.to_csv("df_file.csv", index=False)

"""### Question Answering System"""

import requests
import json
import ipywidgets as widgets
from IPython.display import display, clear_output

# Normally this is a bad idea to include your api_key directly but I am doing a proof of concept
import os
api_key = os.getenv("MY_API_KEY")

# Set out Gemini API endpoint, specifying which gemini model to use, and passing in out API Key credential
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"

# definie a QA Function
def ask_question(question):
    headers = {"Content-Type": "application/json"}
    data = {"contents": [{"parts": [{"text": question}]}]}
    response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        return response.json()["candidates"][0]["content"]["parts"][0]["text"]
    else:
        return f"Error: {response.status_code} - {response.text}"

question = "Help me understand this article"
answer = ask_question(question)
print(answer)

# defining a back-and-forth QA system with the chat log
def ask_question(question, chat_log):
    if len(chat_log) == 0:
        # Set the initial context for the LLM
        chat_log.append({"role": "user", "content": "You are a helpful assistant tasked with answering questions based solely on the content of the provided documents. Use only information found in the text and respond clearly and concisely."})

    # limit the chat_log "memory" up to 5 conversational pairs + initial context
    if len(chat_log) >= 12:
        chat_log = chat_log[-12:]

    # Append in the user question
    chat_log.append({"role": "user", "content": question})

    # Write in and append gemini responses
    gemini_contents = []
    for msg in chat_log:
        role = "user" if msg["role"] == "user" else "model"
        gemini_contents.append({
            "role": role,
            "parts": [{"text": msg["content"]}]})


    headers = {"Content-Type": "application/json"}
    data = {"contents": gemini_contents}
    response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        answer = response.json()["candidates"][0]["content"]["parts"][0]["text"]
        chat_log.append({"role": "assistant", "content": answer})
        return answer, chat_log
    else:
        error_message = f"Error: {response.status_code} - {response.text}"
        chat_log.append({"role": "assistant", "content": error_message})
        return error_message, chat_log

# Widget setup
text_input = widgets.Text(
    placeholder='Type your question here...',
    description='Question:',
    disabled=False)

ask_button = widgets.Button(description="Ask")
output = widgets.Output()
chat_log = []

def on_ask_button_click(b):
    global chat_log  # set up the chat_log as a global variable, defined outside of the function
    with output:
        clear_output()
        question = text_input.value
        if question:
            answer, chat_log = ask_question(question, chat_log)
            for message in chat_log[-12:]:
                print(f"{message['role'].title()}: {message['content']}")
        text_input.value = '' # clear out the input box after asking


# ask button action code
ask_button.on_click(on_ask_button_click)

# Display the widgets
display(text_input, ask_button, output)

"""### Streamlit App"""

!pip install streamlit

import pandas as pd
import numpy as np
import nltk
import re
import contractions

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Load data
df = pd.read_csv("df_file.csv")

# Expand contractions
df['Expanded_Text'] = df['Text'].apply(contractions.fix)

# Clean text
stop_words = set(stopwords.words("english"))

def normalize_document(doc):
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc)
    doc = doc.lower().strip()
    tokens = word_tokenize(doc)
    tokens = [t for t in tokens if t not in stop_words]
    return ' '.join(tokens)

df['Clean_Text'] = df['Expanded_Text'].apply(normalize_document)

from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
import joblib

# Split
X_train, X_test, y_train, y_test = train_test_split(df['Clean_Text'], df['Label'], test_size=0.2, random_state=42)

# Vectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=60166)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Train model
model = LinearSVC()
model.fit(X_train_vec, y_train)

# Save model and vectorizer
joblib.dump(model, "best_text_classifier.pkl")
joblib.dump(vectorizer, "text_vectorizer.pkl")

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from nltk.tokenize import sent_tokenize

# Vectorize full data for KMeans
X_full = vectorizer.transform(df['Clean_Text'])
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X_full)
df['Cluster'] = kmeans.labels_

# Summarization
def summarize_article(text, top_n=3):
    sentences = sent_tokenize(text)
    if len(sentences) < top_n:
        return text
    sentence_vectors = TfidfVectorizer().fit_transform(sentences)
    sim_matrix = cosine_similarity(sentence_vectors)
    scores = nx.pagerank(nx.from_numpy_array(sim_matrix))
    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    return " ".join([s for _, s in ranked[:top_n]])

df['Summary'] = df['Text'].apply(summarize_article)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
import joblib

# Cleaned text column
X = df['Clean_Text']
y = df['Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize CLEAN text
vectorizer = TfidfVectorizer(stop_words='english', max_features=60166)
X_train_vec = vectorizer.fit_transform(X_train)  # <- this fits AND transforms
X_test_vec = vectorizer.transform(X_test)

# Train model
model = LinearSVC()
model.fit(X_train_vec, y_train)

# Save both
joblib.dump(model, "best_text_classifier.pkl")
joblib.dump(vectorizer, "text_vectorizer.pkl")

from google.colab import files
files.download("best_text_classifier.pkl")
files.download("text_vectorizer.pkl")

df.to_csv("df_file.csv", index=False)

# Download files to use in your Streamlit app
from google.colab import files

files.download("best_text_classifier.pkl")
files.download("text_vectorizer.pkl")
files.download("df_file.csv")

import pandas as pd
import numpy as np
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
import joblib

nltk.download('punkt')
nltk.download('stopwords')

# Load your cleaned DataFrame
df = pd.read_csv("df_file.csv")

# Normalize the documents (like in your app)
stop_words = set(stopwords.words("english"))

def normalize_document(doc):
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', str(doc))
    doc = doc.lower().strip()
    tokens = word_tokenize(doc)
    tokens = [t for t in tokens if t not in stop_words]
    return ' '.join(tokens)

df['Clean_Text'] = df['Text'].apply(normalize_document)

# Train-test split
X = df['Clean_Text']
y = df['Label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit vectorizer on training data
vectorizer = TfidfVectorizer(stop_words='english', max_features=60166)
X_train_vec = vectorizer.fit_transform(X_train)

# Fit model
model = LinearSVC()
model.fit(X_train_vec, y_train)

# Save both vectorizer and model
joblib.dump(model, "best_text_classifier.pkl")
joblib.dump(vectorizer, "text_vectorizer.pkl")

# Download to your machine
from google.colab import files
files.download("best_text_classifier.pkl")
files.download("text_vectorizer.pkl")

